{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSUMER COMPLAINTS\n",
    "\n",
    "# The COMPLAINTS file contains all safety-related defect complaints received \n",
    "# by NHTSA since January 1, 1995.\n",
    "\n",
    "# File characteristics:\n",
    "\n",
    "# -  All the records are TAB delimited\n",
    "# -  All dates are in YYYYMMDD format\n",
    "\n",
    "# -  Maximum record length: 2824\n",
    "\n",
    "# Change log:\n",
    "# 1. Fields 21 - 46 were added on Sept. 14, 2007\n",
    "# 2. Changed flat file extension from .lst to .txt on Sept. 14, 2007\n",
    "# 3. Field 47 was added on Oct. 15, 2007\n",
    "# 4. Field 34 was changed from CHAR(20) to CHAR(30) on Aug. 08, 2008\n",
    "# 5. Field 18 was changed from NUMBER(6) to NUMBER(7) on Jun. 18, 2010\n",
    "# 6. Complaint Type 'MIVQ' was added to Field 21 list on Mar. 21, 2013\n",
    "# 7. Complaint Type 'MAVQ' was added to Field 21 list on Jan. 17, 2014\n",
    "# 8. Field 48 was added on Apr. 24, 2014\n",
    "# 9. Field 49 was added on Sept. 29, 2015\n",
    "# 10. Flat file content changes May 17 - June 17, 2021\n",
    "# 11. Information message updated on June 28, 2021\n",
    "\n",
    "# =======\n",
    "# May 17 - June 17, 2021 - Flat file content changes.\n",
    "# * Previously blank Y/N fields (such as crash or fire) will now show as N.\n",
    "# * Previously blank values for numeric fields will now show as zero.\n",
    "# * Manufacturer name, make, model and component name of the product(s) in a complaint may have changed over time and the new flat file will now reflect them.\n",
    "# * If a complaint had multiple dealer contacts, only a single dealer's contact information will now show.\n",
    "# * Additional minor differences due to data cleanup in a relatively small number of records in the flat file.\n",
    "# =======\n",
    "# June 28, 2021 - Flat file content changes.\n",
    "# The NHTSA system that generates the complaints file underwent an update during the weekend of May 16-17, 2021. The update has caused discrepancies between the contents of the complaints file between the version posted on Friday, May 15, 2021 (before the system update) and the versions posted daily since Monday, May 17, 2021 (after the system update) and continuing to date. Lookup of complaints on the NHTSA public website are unaffected.\n",
    "# We are working to resolve the issue. In the meantime, we will continue to publish data daily as it will contain newly received complaints by NHTSA. The complaint data is included in the single FLAT_CMPL.zip and the COMPLAINTS_RECEIVED_YYYY-YYYY.zip files, which contain the same data broken down into 5-year chunks by received date. Note that the format of the complaint files has not and will not change. Once resolved, you may still see minor data differences between the latest version and that from prior to the system update.\n",
    "# We will continue to update this message as needed until the issue is resolved.\n",
    "# =======\n",
    "\n",
    "# Last updated: June 28, 2021\n",
    "\n",
    "# FIELDS:\n",
    "# =======\n",
    "\n",
    "# Field#  Name              Type/Size     Description\n",
    "# ------  ---------         ---------     --------------------------------------\n",
    "# 1       CMPLID            CHAR(9)       NHTSA'S INTERNAL UNIQUE SEQUENCE NUMBER.\n",
    "#                                         IS AN UPDATEABLE FIELD,THUS DATA FOR A\n",
    "#                                         GIVEN RECORD POTENTIALLY COULD CHANGE FROM\n",
    "#                                         ONE DATA OUTPUT FILE TO THE NEXT.\n",
    "# 2       ODINO             CHAR(9)       NHTSA'S INTERNAL REFERENCE NUMBER.\n",
    "#                                         THIS NUMBER MAY BE REPEATED FOR\n",
    "#                                         MULTIPLE COMPONENTS.\n",
    "#                                         ALSO, IF LDATE IS PRIOR TO DEC 15, 2002,\n",
    "#                                         THIS NUMBER MAY BE REPEATED FOR MULTIPLE\n",
    "#                                         PRODUCTS OWNED BY THE SAME COMPLAINANT.\n",
    "# 3       MFR_NAME          CHAR(40)      MANUFACTURER'S NAME\n",
    "# 4       MAKETXT           CHAR(25)      VEHICLE/EQUIPMENT MAKE\n",
    "# 5       MODELTXT          CHAR(256)     VEHICLE/EQUIPMENT MODEL\n",
    "# 6       YEARTXT           CHAR(4)       MODEL YEAR, 9999 IF UNKNOWN or N/A\n",
    "# 7       CRASH             CHAR(1)       WAS VEHICLE INVOLVED IN A CRASH, 'Y' OR 'N'\n",
    "# 8       FAILDATE          CHAR(8)       DATE OF INCIDENT (YYYYMMDD)\n",
    "# 9       FIRE              CHAR(1)       WAS VEHICLE INVOLVED IN A FIRE 'Y' OR 'N'\n",
    "# 10      INJURED           NUMBER(2)     NUMBER OF PERSONS INJURED\n",
    "# 11      DEATHS            NUMBER(2)     NUMBER OF FATALITIES\n",
    "# 12      COMPDESC          CHAR(128)     SPECIFIC COMPONENT'S DESCRIPTION\n",
    "# 13      CITY              CHAR(30)      CONSUMER'S CITY\n",
    "# 14      STATE             CHAR(2)       CONSUMER'S STATE CODE\n",
    "# 15      VIN               CHAR(11)      VEHICLE'S VIN#\n",
    "# 16      DATEA             CHAR(8)       DATE ADDED TO FILE (YYYYMMDD)\n",
    "# 17      LDATE             CHAR(8)       DATE COMPLAINT RECEIVED BY NHTSA (YYYYMMDD)\n",
    "# 18      MILES             NUMBER(7)     VEHICLE MILEAGE AT FAILURE\n",
    "# 19      OCCURENCES        NUMBER(4)     NUMBER OF OCCURRENCES\n",
    "# 20      CDESCR            CHAR(2048)    DESCRIPTION OF THE COMPLAINT\n",
    "# 21      CMPL_TYPE         CHAR(4)       SOURCE OF COMPLAINT CODE:\n",
    "#                                           CAG  =CONSUMER ACTION GROUP\n",
    "#                                           CON  =FORWARDED FROM A CONGRESSIONAL OFFICE\n",
    "#                                           DP   =DEFECT PETITION,RESULT OF A DEFECT PETITION\n",
    "#                                           EVOQ =HOTLINE VOQ\n",
    "#                                           EWR  =EARLY WARNING REPORTING\n",
    "#                                           INS  =INSURANCE COMPANY\n",
    "#                                           IVOQ =NHTSA WEB SITE\n",
    "#                                           LETR =CONSUMER LETTER\n",
    "#                                           MAVQ =NHTSA MOBILE APP\n",
    "#                                           MIVQ =NHTSA MOBILE APP\n",
    "#                                           MVOQ =OPTICAL MARKED VOQ\n",
    "#                                           RC   =RECALL COMPLAINT,RESULT OF A RECALL INVESTIGATION\n",
    "#                                           RP   =RECALL PETITION,RESULT OF A RECALL PETITION\n",
    "#                                           SVOQ =PORTABLE SAFETY COMPLAINT FORM (PDF)\n",
    "#                                           VOQ  =NHTSA VEHICLE OWNERS QUESTIONNAIRE\n",
    "# 22      POLICE_RPT_YN     CHAR(1)       WAS INCIDENT REPORTED TO POLICE 'Y' OR 'N'\n",
    "# 23      PURCH_DT          CHAR(8)       DATE PURCHASED (YYYYMMDD)\n",
    "# 24      ORIG_OWNER_YN     CHAR(1)       WAS ORIGINAL OWNER 'Y' OR 'N'\n",
    "# 25      ANTI_BRAKES_YN    CHAR(1)       ANTI-LOCK BRAKES 'Y' OR 'N'\n",
    "# 26      CRUISE_CONT_YN    CHAR(1)       CRUISE CONTROL 'Y' OR 'N'\n",
    "# 27      NUM_CYLS          NUMBER(2)     NUMBER OF CYLINDERS\n",
    "# 28      DRIVE_TRAIN       CHAR(4)       DRIVE TRAIN TYPE [AWD,4WD,FWD,RWD]\n",
    "# 29      FUEL_SYS          CHAR(4)       FUEL SYSTEM CODE:\n",
    "#                                            FI =FUEL INJECTION\n",
    "#                                            TB =TURBO\n",
    "# 30      FUEL_TYPE         CHAR(4)       FUEL TYPE CODE:\n",
    "#                                            BF =BIFUEL\n",
    "#                                            CN =CNG/LPG\n",
    "#                                            DS =DIESEL\n",
    "#                                            GS =GAS\n",
    "#                                            HE =HYBRID ELECTRIC\n",
    "# 31      TRANS_TYPE        CHAR(4)       VEHICLE TRANSMISSION TYPE [AUTO, MAN]\n",
    "# 32      VEH_SPEED         NUMBER(3)     VEHICLE SPEED\n",
    "# 33      DOT               CHAR(20)      DEPARTMENT OF TRANSPORTATION TIRE IDENTIFIER\n",
    "# 34      TIRE_SIZE         CHAR(30)      TIRE SIZE\n",
    "# 35      LOC_OF_TIRE       CHAR(4)       LOCATION OF TIRE CODE:\n",
    "#                                            FSW =DRIVER SIDE FRONT\n",
    "#                                            DSR =DRIVER SIDE REAR\n",
    "#                                            FTR =PASSENGER SIDE FRONT\n",
    "#                                            PSR =PASSENGER SIDE REAR\n",
    "#                                            SPR =SPARE\n",
    "# 36      TIRE_FAIL_TYPE    CHAR(4)       TYPE OF TIRE FAILURE CODE:\n",
    "#                                            BST =BLISTER\n",
    "#                                            BLW =BLOWOUT\n",
    "#                                            TTL =CRACK\n",
    "#                                            OFR =OUT OF ROUND\n",
    "#                                            TSW =PUNCTURE\n",
    "#                                            TTR =ROAD HAZARD\n",
    "#                                            TSP =TREAD SEPARATION\n",
    "# 37      ORIG_EQUIP_YN     CHAR(1)       WAS PART ORIGINAL EQUIPMENT 'Y' OR 'N'\n",
    "# 38      MANUF_DT          CHAR(8)       DATE OF MANUFACTURE (YYYYMMDD)\n",
    "# 39      SEAT_TYPE         CHAR(4)       TYPE OF CHILD SEAT CODE:\n",
    "#                                            B  =BOOSTER\n",
    "#                                            C  =CONVERTIBLE\n",
    "#                                            I  =INFANT\n",
    "#                                            IN =INTEGRATED\n",
    "#                                            TD =TODDLER\n",
    "# 40     RESTRAINT_TYPE     CHAR(4)       INSTALLATION SYSTEM CODE;\n",
    "#                                            A =VEHICLE SAFETY BELT\n",
    "#                                            B =LATCH SYSTEM\n",
    "# 41     DEALER_NAME        CHAR(40)      DEALER'S NAME\n",
    "# 42     DEALER_TEL         CHAR(20)      DEALER'S TELEPHONE NUMBER\n",
    "# 43     DEALER_CITY        CHAR(30)      DEALER'S CITY\n",
    "# 44     DEALER_STATE       CHAR(2)       DEALER'S STATE CODE\n",
    "# 45     DEALER_ZIP         CHAR(10)      DEALER'S ZIPCODE\n",
    "# 46     PROD_TYPE          CHAR(4)       PRODUCT TYPE CODE:\n",
    "#                                            V =VEHICLE\n",
    "#                                            T =TIRES\n",
    "#                                            E =EQUIPMENT\n",
    "#                                            C =CHILD RESTRAINT\n",
    "# 47     REPAIRED_YN        CHAR(1)       WAS DEFECTIVE TIRE REPAIRED 'Y' OR 'N'\n",
    "# 48     MEDICAL_ATTN       CHAR(1)       WAS MEDICAL ATTENTION REQUIRED 'Y' OR 'N'\n",
    "# 49     VEHICLES_TOWED_YN  CHAR(1)       WAS VEHICLE TOWED 'Y' OR 'N'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# https://www.nhtsa.gov/nhtsa-datasets-and-apis#recalls\n",
    "# read in C:\\Repo\\SIADs_Audio_Text_SRS\\Example\\COMPLAINTS_RECEIVED_2025-2025.txt into a pandas dataframe, where the columns are RCL\n",
    "df_complaints = pd.read_csv(\"C:\\\\Repo\\\\SIADs_Audio_Text_SRS\\\\Datasets\\\\COMPLAINTS_RECEIVED_2025-2025.txt\", sep='\\t', header=None, index_col=0)\n",
    "df_complaints.columns = ['ODINO', 'MFR_NAME', 'MAKETXT', 'MODELTXT', 'YEARTXT', 'CRASH', 'FAILDATE', 'FIRE', 'INJURED', 'DEATHS', 'COMPDESC', 'CITY', 'STATE', 'VIN', 'DATEA', 'LDATE', 'MILES', 'OCCURENCES', 'CDESCR', 'CMPL_TYPE', 'POLICE_RPT_YN', 'PURCH_DT', 'ORIG_OWNER_YN', 'ANTI_BRAKES_YN', 'CRUISE_CONT_YN', 'NUM_CYLS', 'DRIVE_TRAIN', 'FUEL_SYS', 'FUEL_TYPE',\n",
    "              'TRANS_TYPE', 'VEH_SPEED', 'DOT', 'TIRE_SIZE', 'LOC_OF_TIRE', 'TIRE_FAIL_TYPE', 'ORIG_EQUIP_YN', 'MANUF_DT', 'SEAT_TYPE', 'RESTRAINT_TYPE', 'DEALER_NAME', 'DEALER_TEL', 'DEALER_CITY', 'DEALER_STATE', 'DEALER_ZIP', 'PROD_TYPE', 'REPAIRED_YN', 'MEDICAL_ATTN', 'VEHICLES_TOWED_YN']\n",
    "\n",
    "summary = df_complaints.describe()\n",
    "display(summary)\n",
    "\n",
    "deadly_complaints = df_complaints[df_complaints[\"DEATHS\"] > 0]\n",
    "#print(deadly_complaints)\n",
    "\n",
    "# print any of the complaints that lead to a death\n",
    "display(deadly_complaints[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"DEATHS\", \"CDESCR\"]])\n",
    "# print the tesla complaints that lead to a death\n",
    "print(deadly_complaints[deadly_complaints[\"MFR_NAME\"] == \"Tesla, Inc.\"][\"CDESCR\"].values)\n",
    "\n",
    "# display columns 11 through 25\n",
    "#display(df_complaints)\n",
    "\n",
    "sub_df = df_complaints[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]]\n",
    "display(sub_df)\n",
    "#display(read_complaints_received())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALLS\n",
    "\n",
    "# The RECALL file contains all NHTSA safety-related defect and compliance \n",
    "# campaigns since 1967.\n",
    "\n",
    "# File characteristics:\n",
    "\n",
    "# -  All the records are TAB delimited\n",
    "# -  All dates are in YYYYMMDD format\n",
    "\n",
    "# -  Maximum Record length: 9109\n",
    "\n",
    "# Change log:\n",
    "# 1.Field# 23 added as of Sept. 14, 2007\n",
    "# 2.Changed flat file extension from .lst to .txt as of Sept. 14, 2007\n",
    "# 3.Field# 24 added as of March 14, 2008\n",
    "# 4.Field#s 25,26,27 added as of March 23, 2020\n",
    "\n",
    "# Last Updated March 23, 2020\n",
    "\n",
    "\n",
    "# FIELDS:\n",
    "# =======\n",
    "\n",
    "# Field#   Name                Type/Size   Description                      \n",
    "# ------   ---------           ---------   --------------------------------------\n",
    "# 1        RECORD_ID           NUMBER(9)   RUNNING SEQUENCE NUMBER, \n",
    "#                                           WHICH UNIQUELY IDENTIFIES THE RECORD.\n",
    "# 2        CAMPNO              CHAR(12)    NHTSA CAMPAIGN NUMBER\n",
    "# 3        MAKETXT             CHAR(25)    VEHICLE/EQUIPMENT MAKE\n",
    "# 4        MODELTXT            CHAR(256)   VEHICLE/EQUIPMENT MODEL\n",
    "# 5        YEARTXT             CHAR(4)     MODEL YEAR, 9999 IF UNKNOWN or N/A\n",
    "# 6        MFGCAMPNO           CHAR(20)    MFR CAMPAIGN NUMBER\n",
    "# 7        COMPNAME            CHAR(256)   COMPONENT DESCRIPTION\n",
    "# 8        MFGNAME             CHAR(40)    MANUFACTURER THAT FILED DEFECT/NONCOMPLIANCE REPORT\n",
    "# 9        BGMAN               CHAR(8)     BEGIN DATE OF MANUFACTURING\n",
    "# 10       ENDMAN              CHAR(8)     END DATE OF MANUFACTURING\n",
    "# 11       RCLTYPECD           CHAR(4)     VEHICLE, EQUIPMENT OR TIRE REPORT\n",
    "# 12       POTAFF              NUMBER(9)   POTENTIAL NUMBER OF UNITS AFFECTED               \n",
    "# 13       ODATE               CHAR(8)     DATE OWNER NOTIFIED BY MFR\n",
    "# 14       INFLUENCED_BY       CHAR(4)     RECALL INITIATOR (MFR/OVSC/ODI)\n",
    "# 15       MFGTXT              CHAR(40)    MANUFACTURERS OF RECALLED VEHICLES/PRODUCTS\n",
    "# 16       RCDATE              CHAR(8)     REPORT RECEIVED DATE\n",
    "# 17       DATEA               CHAR(8)     RECORD CREATION DATE\n",
    "# 18       RPNO                CHAR(3)     REGULATION PART NUMBER\n",
    "# 19       FMVSS               CHAR(10)    FEDERAL MOTOR VEHICLE SAFETY STANDARD NUMBER\n",
    "# 20       DESC_DEFECT         CHAR(2000)  DEFECT SUMMARY\n",
    "# 21       CONEQUENCE_DEFECT   CHAR(2000)  CONSEQUENCE SUMMARY\t\n",
    "# 22       CORRECTIVE_ACTION   CHAR(2000)  CORRECTIVE SUMMARY\n",
    "# 23       NOTES               CHAR(2000)  RECALL NOTES\n",
    "# 24       RCL_CMPT_ID         CHAR(27)    NUMBER THAT UNIQUELY IDENTIFIES A RECALLED COMPONENT.\n",
    "# 25       MFR_COMP_NAME       CHAR(50)    MANUFACTURER-SUPPLIED COMPONENT NAME\n",
    "# 26       MFR_COMP_DESC       CHAR(200)   MANUFACTURER-SUPPLIED COMPONENT DESCRIPTION\n",
    "# 27       MFR_COMP_PTNO       CHAR(100)   MANUFACTURER-SUPPLIED COMPONENT PART NUMBER\n",
    "\n",
    "# read in C:\\Repo\\SIADs_Audio_Text_SRS\\Datasets\\FLAT_RCL.txt\n",
    "# there are only 24 columns in the file, so we need to specify the column names\n",
    "# https://www.nhtsa.gov/nhtsa-datasets-and-apis#recalls\n",
    "df = pd.read_csv(\"C:\\\\Repo\\\\SIADs_Audio_Text_SRS\\\\Datasets\\\\FLAT_RCL.txt\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "# use the column names listed above\n",
    "df.columns = ['RECORD_ID', 'CAMPNO', 'MAKETXT', 'MODELTXT', 'YEARTXT', 'MFGCAMPNO', 'COMPNAME', 'MFGNAME', 'BGMAN', 'ENDMAN', 'RCLTYPECD', 'POTAFF', 'ODATE', 'INFLUENCED_BY', 'MFGTXT', 'RCDATE', 'DATEA', 'RPNO', 'FMVSS', 'DESC_DEFECT', 'CONEQUENCE_DEFECT', 'CORRECTIVE_ACTION', 'NOTES', 'RCL_CMPT_ID', 'MFR_COMP_NAME', 'MFR_COMP_DESC', 'MFR_COMP_PTNO']\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# create a function that will be called from a apply lambda function to process the text from a dataframe with the \"CDESCR\" column\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process text by tokenizing, removing stop words, and stemming\n",
    "    \"\"\"\n",
    "    #print(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # make sure the text is not empty and or nan\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    # tokenize the text and remove stop words and stem the words\n",
    "    content_cleaned = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word not in stop_words]\n",
    "\n",
    "    # remove the punctuation from the content_cleaned list\n",
    "    content_cleaned = [word for word in content_cleaned if word.isalnum()]\n",
    "    \n",
    "    return content_cleaned\n",
    "\n",
    "# process the text in the \"CDESCR\" column and create a new column \"CDESCR_CLEANED\" with the processed text\n",
    "df_complaints[\"CDESCR_CLEANED\"] = df_complaints[\"CDESCR\"].apply(lambda x: process_text(x))\n",
    "display(df_complaints[[\"CDESCR\", \"CDESCR_CLEANED\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common words in the \"CDESCR_CLEANED\" column\n",
    "from collections import Counter\n",
    "\n",
    "# create a Counter object from the \"CDESCR_CLEANED\" column\n",
    "word_counter = Counter([word for words in df_complaints[\"CDESCR_CLEANED\"] for word in words])\n",
    "\n",
    "# get the 10 most common words\n",
    "most_common_words = word_counter.most_common(10)\n",
    "print(most_common_words)\n",
    "\n",
    "# get the 10 least common words\n",
    "least_common_words = word_counter.most_common()[:-10:-1]\n",
    "print(least_common_words)\n",
    "\n",
    "# get the 10 most common words in the \"CDESCR_CLEANED\" column for complaints that lead to a death\n",
    "deadly_complaints = df_complaints[df_complaints[\"DEATHS\"] > 0]\n",
    "word_counter_deadly = Counter([word for words in deadly_complaints[\"CDESCR_CLEANED\"] for word in words])\n",
    "most_common_words_deadly = word_counter_deadly.most_common(10)\n",
    "print(most_common_words_deadly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nan values from the \"CDESCR\" column and put it into a new dataframe named \"df_complaints_no_nan\"\n",
    "df_complaints_no_nan = df_complaints.dropna(subset=[\"CDESCR\"])\n",
    "# find any instances of the word \"diagnostic\" or \"DTC\" in the \"CDESCR\" column, DTC stands for Diagnostic Trouble Code and is used in the automotive industry for identifying issues with a vehicle\n",
    "diagnostic_complaints = df_complaints_no_nan[df_complaints_no_nan[\"CDESCR\"].str.contains(\"diagnostic|DTC\", case=False)]\n",
    "display(diagnostic_complaints[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\", \"CDESCR_CLEANED\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the df_complaints dataframe into a test, train, and validation set with a 70/20/10 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42\n",
    "train_size = 0.7\n",
    "test_size = 0.2\n",
    "validation_size = 0.1\n",
    "\n",
    "\n",
    "train, test = train_test_split(df_complaints, test_size=(1-train_size), random_state=random_state)\n",
    "test, validation = train_test_split(test, test_size=(validation_size/(1-train_size)), random_state=random_state)\n",
    "print(df_complaints.shape)\n",
    "print(train.shape, test.shape, validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the \"CDESCR_CLEANED\" column using the TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# set the number of dimensions to reduce the vectorized data to\n",
    "num_dimensions = 100\n",
    "\n",
    "\n",
    "# fit the vectorizer on the \"CDESCR_CLEANED\" column and transform the \"CDESCR_CLEANED\" column into a vectorized format\n",
    "# apply a lambda function to join the list of words in the \"CDESCR_CLEANED\" column into a string\n",
    "X_train = vectorizer.fit_transform(train[\"CDESCR_CLEANED\"].apply(lambda x: \" \".join(x)))\n",
    "X_test = vectorizer.transform(test[\"CDESCR_CLEANED\"].apply(lambda x: \" \".join(x)))\n",
    "X_validation = vectorizer.transform(validation[\"CDESCR_CLEANED\"].apply(lambda x: \" \".join(x)))\n",
    "print(X_train.shape, X_test.shape, X_validation.shape)\n",
    "\n",
    "# perform LSA on the vectorized data to reduce the dimensionality\n",
    "lsa = TruncatedSVD(n_components=num_dimensions, random_state=random_state)\n",
    "complaints_vectorized_train = lsa.fit_transform(X_train)\n",
    "vectorized_test = lsa.transform(X_test)\n",
    "complaints_vectorized_validation = lsa.transform(X_validation)\n",
    "print(complaints_vectorized_train.shape, vectorized_test.shape, complaints_vectorized_validation.shape)\n",
    "# print out the words that correspond to the first 10 dimensions of the LSA\n",
    "\n",
    "# create a list of unique manufacturers in the \"MFR_NAME\" column\n",
    "list_of_manufacturers = df_complaints[\"MFR_NAME\"].unique()\n",
    "#print(list_of_manufacturers)\n",
    "\n",
    "# Find the cosine similarity between the first complaint in the test set and all the complaints in the training set\n",
    "# get the first complaint in the test set\n",
    "complaint_test = vectorized_test[5].reshape(1, -1)\n",
    "# get the cosine similarity between the first complaint in the test set and all the complaints in the training set\n",
    "cosine_similarities = cosine_similarity(complaint_test, complaints_vectorized_train)\n",
    "# get the index of the most similar complaint in the training set\n",
    "most_similar_index = cosine_similarities.argmax()\n",
    "print(most_similar_index)\n",
    "# get the most similar complaint in the training set\n",
    "most_similar_complaint_train = train.iloc[most_similar_index]\n",
    "print(most_similar_complaint_train[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint_train[\"CDESCR\"])\n",
    "# get the most similar complaint in the test set\n",
    "most_similar_complaint_test = test.iloc[5]\n",
    "print(most_similar_complaint_test[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint_test[\"CDESCR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query text to vectorize and find similar complaints to\n",
    "query_text = \"EV not charging\"\n",
    "\n",
    "# process the query text\n",
    "query_text_cleaned = process_text(query_text)\n",
    "# vectorize the query text\n",
    "query_vectorized = vectorizer.transform([\" \".join(query_text_cleaned)])\n",
    "# reduce the dimensionality of the query vector\n",
    "query_vectorized_lsa = lsa.transform(query_vectorized)\n",
    "# find the cosine similarity between the query vector and all the complaints in the training set\n",
    "cosine_similarities_query = cosine_similarity(query_vectorized_lsa, complaints_vectorized_train)\n",
    "# get the index of the most similar complaint in the training set\n",
    "most_similar_index_query = cosine_similarities_query.argmax()\n",
    "# get the most similar complaint in the training set\n",
    "most_similar_complaint_train_query = train.iloc[most_similar_index_query]\n",
    "print(most_similar_complaint_train_query[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint_train_query[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionalize the process of finding similar complaints to a query text\n",
    "def find_similar_complaint(query_text:str, vectorizer, lsa, train):\n",
    "    \"\"\"\n",
    "    Find the most similar complaint to a query text in the training set\n",
    "    \"\"\"\n",
    "    # process the query text\n",
    "    query_text_cleaned = process_text(query_text)\n",
    "    # vectorize the query text\n",
    "    query_vectorized = vectorizer.transform([\" \".join(query_text_cleaned)])\n",
    "    # reduce the dimensionality of the query vector\n",
    "    query_vectorized_lsa = lsa.transform(query_vectorized)\n",
    "    # find the cosine similarity between the query vector and all the complaints in the training set\n",
    "    cosine_similarities_query = cosine_similarity(query_vectorized_lsa, complaints_vectorized_train)\n",
    "    # get the index of the most similar complaint in the training set\n",
    "    most_similar_index_query = cosine_similarities_query.argmax()\n",
    "    # get the most similar complaint in the training set\n",
    "    most_similar_complaint_train_query = train.iloc[most_similar_index_query]\n",
    "    return most_similar_complaint_train_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_stuff = \"rattling noise when driving slowly\"\n",
    "most_similar_complaint = find_similar_complaint(find_similar_stuff, vectorizer, lsa, train)\n",
    "print(most_similar_complaint[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self, df, column_name:str):\n",
    "        # set the random state for reproducibility\n",
    "        self.random_state = 42\n",
    "        # create a TfidfVectorizer object\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        # set the number of dimensions to reduce the vectorized data to\n",
    "        self.num_dimensions = 100\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.column_name_cleaned = column_name + \"_CLEANED\"\n",
    "        # create variables to store the training, test, and validation sets for class functions\n",
    "        self.column_name_cleaned = None\n",
    "        self.df_train = None\n",
    "        self.df_test = None\n",
    "        self.df_validation = None\n",
    "        self.x_train_vect = None\n",
    "        self.x_test_vect = None\n",
    "        self.x_validation_vect = None\n",
    "        self.lsa = None\n",
    "        self.vectorized_train = None\n",
    "        self.vectorized_test = None\n",
    "        self.vectorized_validation = None\n",
    "\n",
    "    # create a function that will be called from a apply lambda function to process the text from a dataframe with the \"CDESCR\" column\n",
    "    def process_text(text):\n",
    "        \"\"\"\n",
    "        Process text by tokenizing, removing stop words, and stemming\n",
    "        \"\"\"\n",
    "        #print(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "        # make sure the text is not empty and or nan\n",
    "        if not text or pd.isna(text):\n",
    "            return []\n",
    "\n",
    "        # tokenize the text and remove stop words and stem the words\n",
    "        content_cleaned = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word not in stop_words]\n",
    "\n",
    "        # remove the punctuation from the content_cleaned list\n",
    "        content_cleaned = [word for word in content_cleaned if word.isalnum()]\n",
    "        \n",
    "        return content_cleaned\n",
    "    \n",
    "    def process_dataframe(self, train_size=0.7, test_size=0.2, validation_size=0.1):\n",
    "        \"\"\"\n",
    "        Process the text in the \"CDESCR\" column and create a new column \"CDESCR_CLEANED\" with the processed text\n",
    "        \"\"\"\n",
    "        # get current working directory\n",
    "        cwd = os.getcwd()\n",
    "        # change the Example folder to Datasets folder in the cwd path\n",
    "        desired_save_path = cwd.replace(\"Example\", \"Datasets\")\n",
    "        # create a folder path to save the pickle files\n",
    "        if not os.path.exists(desired_save_path):\n",
    "            os.makedirs(desired_save_path)\n",
    "\n",
    "        # check to see if there is a pickle file for the dataframe with the processed text\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_df.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_df.pkl\", \"rb\") as f:\n",
    "                self.df = pickle.load(f)\n",
    "        else:\n",
    "            # process the text in the \"CDESCR\" column and create a new column \"CDESCR_CLEANED\" with the processed text\n",
    "            self.df[self.column_name_cleaned] = self.df[self.column_name].apply(lambda x: TextClassifier.process_text(x))\n",
    "            # create a pickle file for the dataframe with the processed text\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_df.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.df, f)\n",
    "\n",
    "        # split the df_complaints dataframe into a test, train, and validation set with a 70/20/10 split\n",
    "        self.df_train, self.df_test = train_test_split(self.df, test_size=(1-train_size), random_state=self.random_state)\n",
    "        self.df_test, self.df_validation = train_test_split(self.df_test, test_size=(validation_size/(1-train_size)), random_state=self.random_state)\n",
    "\n",
    "        # check to see if there is a pickle file for the vectorizer\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorizer.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorizer.pkl\", \"rb\") as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "        else:\n",
    "            # fit the vectorizer on the \"CDESCR_CLEANED\" column and transform the \"CDESCR_CLEANED\" column into a vectorized format\n",
    "            # apply a lambda function to join the list of words in the \"CDESCR_CLEANED\" column into a string\n",
    "            self.x_train_vect = self.vectorizer.fit_transform(self.df_train[self.column_name_cleaned].apply(lambda x: \" \".join(x)))\n",
    "            self.x_test_vect = self.vectorizer.transform(self.df_test[self.column_name_cleaned].apply(lambda x: \" \".join(x)))\n",
    "            self.x_validation_vect = self.vectorizer.transform(self.df_validation[self.column_name_cleaned].apply(lambda x: \" \".join(x)))        \n",
    "            #print(self.x_train_vect.shape, self.x_test_vect.shape, self.x_validation_vect.shape)\n",
    "            # create a pickle file for the vectorizer\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorizer.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.vectorizer, f)\n",
    "\n",
    "        # check to see if there is a pickle file for the lsa\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_lsa.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_lsa.pkl\", \"rb\") as f:\n",
    "                self.lsa = pickle.load(f)\n",
    "        else:\n",
    "            # perform LSA on the vectorized data to reduce the dimensionality\n",
    "            self.lsa = TruncatedSVD(n_components=self.num_dimensions, random_state=self.random_state)\n",
    "\n",
    "        # check to see if there is a pickle file for the vectorized training data\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorized_train.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_train.pkl\", \"rb\") as f:\n",
    "                self.complaints_vectorized_train = pickle.load(f)\n",
    "        else:\n",
    "            self.complaints_vectorized_train = self.lsa.fit_transform(self.x_train_vect)\n",
    "            # create a pickle file for the vectorized training data\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_train.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.complaints_vectorized_train, f)\n",
    "        \n",
    "        # check to see if there is a pickle file for the vectorized test data\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorized_test.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_test.pkl\", \"rb\") as f:\n",
    "                self.vectorized_test = pickle.load(f)\n",
    "        else:\n",
    "            self.vectorized_test = self.lsa.transform(self.x_test_vect)\n",
    "            # create a pickle file for the vectorized test data\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_test.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.vectorized_test, f)\n",
    "\n",
    "        # check to see if there is a pickle file for the vectorized validation data\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorized_validation.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_validation.pkl\", \"rb\") as f:\n",
    "                self.complaints_vectorized_validation = pickle.load(f)\n",
    "        else:\n",
    "            self.complaints_vectorized_validation = self.lsa.transform(self.x_validation_vect)\n",
    "            # create a pickle file for the vectorized validation data\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_validation.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.complaints_vectorized_validation, f)\n",
    "\n",
    "        return self.df, self.complaints_vectorized_train, self.vectorized_test, self.complaints_vectorized_validation, self.df_train, self.df_test, self.df_validation\n",
    "    \n",
    "    # A functionalize the process of finding similar complaints to a query text\n",
    "    def find_similar_complaint(self, query_text:str):\n",
    "        \"\"\"\n",
    "        Find the most similar complaint to a query text in the training set\n",
    "        \"\"\"\n",
    "        # process the query text\n",
    "        query_text_cleaned = TextClassifier.process_text(query_text)\n",
    "        # vectorize the query text\n",
    "        query_vectorized = self.vectorizer.transform([\" \".join(query_text_cleaned)])\n",
    "        # reduce the dimensionality of the query vector\n",
    "        query_vectorized_lsa = self.lsa.transform(query_vectorized)\n",
    "        # find the cosine similarity between the query vector and all the complaints in the training set\n",
    "        cosine_similarities_query = cosine_similarity(query_vectorized_lsa, self.complaints_vectorized_train)\n",
    "        # get the index of the most similar complaint in the training set\n",
    "        most_similar_index_query = cosine_similarities_query.argmax()\n",
    "        print(most_similar_index_query)\n",
    "        # get the most similar complaint in the training set\n",
    "        most_similar_complaint_train_query = self.df_train.iloc[most_similar_index_query]\n",
    "        return most_similar_complaint_train_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiple issues, passenger window is a very common issue, looked on YouTube and found many videos for it was able to jump with direct power to the pigtail using wires and a battery and was able to roll it up but no explanation for why it ends up having this issue, heater core also a consistent issue, it is obvious with passenger side goes cold and 95% of the fix is a new heater core. Headrests popping, had the safety recall done 02chz28 (Recall S61) and never was mentioned or brought up by local dealer, both seatbelts are not functioning properly, passenger side is the worst, locks up and both are running along the plastic causing it to become deformed and wearing on the edges of the belts. Vehicle has obvious replacements on the front but no accident history. Hood is a different vin number and grill is not original from factory. I found the original photo of the car's first sale post. Odd malfunction is washer fluid can run out with no indicator being triggered for low fluid. \n",
      "4686\n",
      "ODINO                                                11634896\n",
      "MFR_NAME                             Toyota Motor Corporation\n",
      "MAKETXT                                                TOYOTA\n",
      "MODELTXT                                               SIENNA\n",
      "YEARTXT                                                  2024\n",
      "CDESCR      Driver side outside rear view mirror shakes an...\n",
      "Name: 2053782, dtype: object\n",
      "Driver side outside rear view mirror shakes and makes driver nauseous.  Mirror was replaced in December and replacement mirror shakes as well.  Right side (passenger) doesn't shake.  Vehicle now has 6500 miles on it.    Original equipment windshield has a wavy appearance for the right 1/3 of it (in front of the front passenger seat).  Both issues have been brought up with Mitchell Toyota in San Angelo, TX.  The dealer will not replace the driver side mirror again, saying \"some movement is allowable\", even though the passenger mirror doesn't do it at all.  They also said that the windshield waviness is \"normal\".    These issues are in addition to other issues I've had with this Sienna, from moldly HVAC smell that caused my eyes to sting and tear up last week.\n"
     ]
    }
   ],
   "source": [
    "# https://www.nhtsa.gov/nhtsa-datasets-and-apis#recalls\n",
    "# read in C:\\Repo\\SIADs_Audio_Text_SRS\\Example\\COMPLAINTS_RECEIVED_2025-2025.txt into a pandas dataframe, where the columns are RCL\n",
    "df_complaints = pd.read_csv(\"C:\\\\Repo\\\\SIADs_Audio_Text_SRS\\\\Datasets\\\\COMPLAINTS_RECEIVED_2025-2025.txt\", sep='\\t', header=None, index_col=0)\n",
    "df_complaints.columns = ['ODINO', 'MFR_NAME', 'MAKETXT', 'MODELTXT', 'YEARTXT', 'CRASH', 'FAILDATE', 'FIRE', 'INJURED', 'DEATHS', 'COMPDESC', 'CITY', 'STATE', 'VIN', 'DATEA', 'LDATE', 'MILES', 'OCCURENCES', 'CDESCR', 'CMPL_TYPE', 'POLICE_RPT_YN', 'PURCH_DT', 'ORIG_OWNER_YN', 'ANTI_BRAKES_YN', 'CRUISE_CONT_YN', 'NUM_CYLS', 'DRIVE_TRAIN', 'FUEL_SYS', 'FUEL_TYPE',\n",
    "              'TRANS_TYPE', 'VEH_SPEED', 'DOT', 'TIRE_SIZE', 'LOC_OF_TIRE', 'TIRE_FAIL_TYPE', 'ORIG_EQUIP_YN', 'MANUF_DT', 'SEAT_TYPE', 'RESTRAINT_TYPE', 'DEALER_NAME', 'DEALER_TEL', 'DEALER_CITY', 'DEALER_STATE', 'DEALER_ZIP', 'PROD_TYPE', 'REPAIRED_YN', 'MEDICAL_ATTN', 'VEHICLES_TOWED_YN']\n",
    "\n",
    "# create a list of unique manufacturers in the \"MFR_NAME\" column\n",
    "list_of_manufacturers = df_complaints[\"MFR_NAME\"].unique()\n",
    "\n",
    "# testing out the functionilzed code\n",
    "\n",
    "# call the TextClassifier class and create an instance of it as text_classifier\n",
    "# pass in the df_complaints dataframe and the \"CDESCR\" column\n",
    "text_classifier = TextClassifier(df_complaints, \"CDESCR\")\n",
    "# process the text in the \"CDESCR\" column\n",
    "text_classifier.process_dataframe()\n",
    "\n",
    "# use one of the complaints in the test set as a query to find the most similar complaint in the training set\n",
    "complaint_test_query = text_classifier.df_test[\"CDESCR\"].iloc[5]\n",
    "print(complaint_test_query)\n",
    "# find the most similar complaint to the complaint test\n",
    "most_similar_complaint = text_classifier.find_similar_complaint(complaint_test_query)\n",
    "# print the most similar complaint with the below columns\n",
    "print(most_similar_complaint[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7668\n",
      "ODINO                              11640271\n",
      "MFR_NAME                General Motors, LLC\n",
      "MAKETXT                           CHEVROLET\n",
      "MODELTXT                             IMPALA\n",
      "YEARTXT                                2009\n",
      "CDESCR      It rumbles when I start my car \n",
      "Name: 2061455, dtype: object\n",
      "It rumbles when I start my car \n"
     ]
    }
   ],
   "source": [
    "# find the most similar complaint to the complaint test\n",
    "most_similar_complaint = text_classifier.find_similar_complaint(\"Car won't start\")# and makes a clicking noise\")\n",
    "# print the most similar complaint with the below columns\n",
    "print(most_similar_complaint[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
