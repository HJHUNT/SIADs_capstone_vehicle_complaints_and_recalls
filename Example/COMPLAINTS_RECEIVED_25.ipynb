{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSUMER COMPLAINTS\n",
    "\n",
    "# The COMPLAINTS file contains all safety-related defect complaints received \n",
    "# by NHTSA since January 1, 1995.\n",
    "\n",
    "# File characteristics:\n",
    "\n",
    "# -  All the records are TAB delimited\n",
    "# -  All dates are in YYYYMMDD format\n",
    "\n",
    "# -  Maximum record length: 2824\n",
    "\n",
    "# Change log:\n",
    "# 1. Fields 21 - 46 were added on Sept. 14, 2007\n",
    "# 2. Changed flat file extension from .lst to .txt on Sept. 14, 2007\n",
    "# 3. Field 47 was added on Oct. 15, 2007\n",
    "# 4. Field 34 was changed from CHAR(20) to CHAR(30) on Aug. 08, 2008\n",
    "# 5. Field 18 was changed from NUMBER(6) to NUMBER(7) on Jun. 18, 2010\n",
    "# 6. Complaint Type 'MIVQ' was added to Field 21 list on Mar. 21, 2013\n",
    "# 7. Complaint Type 'MAVQ' was added to Field 21 list on Jan. 17, 2014\n",
    "# 8. Field 48 was added on Apr. 24, 2014\n",
    "# 9. Field 49 was added on Sept. 29, 2015\n",
    "# 10. Flat file content changes May 17 - June 17, 2021\n",
    "# 11. Information message updated on June 28, 2021\n",
    "\n",
    "# =======\n",
    "# May 17 - June 17, 2021 - Flat file content changes.\n",
    "# * Previously blank Y/N fields (such as crash or fire) will now show as N.\n",
    "# * Previously blank values for numeric fields will now show as zero.\n",
    "# * Manufacturer name, make, model and component name of the product(s) in a complaint may have changed over time and the new flat file will now reflect them.\n",
    "# * If a complaint had multiple dealer contacts, only a single dealer's contact information will now show.\n",
    "# * Additional minor differences due to data cleanup in a relatively small number of records in the flat file.\n",
    "# =======\n",
    "# June 28, 2021 - Flat file content changes.\n",
    "# The NHTSA system that generates the complaints file underwent an update during the weekend of May 16-17, 2021. The update has caused discrepancies between the contents of the complaints file between the version posted on Friday, May 15, 2021 (before the system update) and the versions posted daily since Monday, May 17, 2021 (after the system update) and continuing to date. Lookup of complaints on the NHTSA public website are unaffected.\n",
    "# We are working to resolve the issue. In the meantime, we will continue to publish data daily as it will contain newly received complaints by NHTSA. The complaint data is included in the single FLAT_CMPL.zip and the COMPLAINTS_RECEIVED_YYYY-YYYY.zip files, which contain the same data broken down into 5-year chunks by received date. Note that the format of the complaint files has not and will not change. Once resolved, you may still see minor data differences between the latest version and that from prior to the system update.\n",
    "# We will continue to update this message as needed until the issue is resolved.\n",
    "# =======\n",
    "\n",
    "# Last updated: June 28, 2021\n",
    "\n",
    "# FIELDS:\n",
    "# =======\n",
    "\n",
    "# Field#  Name              Type/Size     Description\n",
    "# ------  ---------         ---------     --------------------------------------\n",
    "# 1       CMPLID            CHAR(9)       NHTSA'S INTERNAL UNIQUE SEQUENCE NUMBER.\n",
    "#                                         IS AN UPDATEABLE FIELD,THUS DATA FOR A\n",
    "#                                         GIVEN RECORD POTENTIALLY COULD CHANGE FROM\n",
    "#                                         ONE DATA OUTPUT FILE TO THE NEXT.\n",
    "# 2       ODINO             CHAR(9)       NHTSA'S INTERNAL REFERENCE NUMBER.\n",
    "#                                         THIS NUMBER MAY BE REPEATED FOR\n",
    "#                                         MULTIPLE COMPONENTS.\n",
    "#                                         ALSO, IF LDATE IS PRIOR TO DEC 15, 2002,\n",
    "#                                         THIS NUMBER MAY BE REPEATED FOR MULTIPLE\n",
    "#                                         PRODUCTS OWNED BY THE SAME COMPLAINANT.\n",
    "# 3       MFR_NAME          CHAR(40)      MANUFACTURER'S NAME\n",
    "# 4       MAKETXT           CHAR(25)      VEHICLE/EQUIPMENT MAKE\n",
    "# 5       MODELTXT          CHAR(256)     VEHICLE/EQUIPMENT MODEL\n",
    "# 6       YEARTXT           CHAR(4)       MODEL YEAR, 9999 IF UNKNOWN or N/A\n",
    "# 7       CRASH             CHAR(1)       WAS VEHICLE INVOLVED IN A CRASH, 'Y' OR 'N'\n",
    "# 8       FAILDATE          CHAR(8)       DATE OF INCIDENT (YYYYMMDD)\n",
    "# 9       FIRE              CHAR(1)       WAS VEHICLE INVOLVED IN A FIRE 'Y' OR 'N'\n",
    "# 10      INJURED           NUMBER(2)     NUMBER OF PERSONS INJURED\n",
    "# 11      DEATHS            NUMBER(2)     NUMBER OF FATALITIES\n",
    "# 12      COMPDESC          CHAR(128)     SPECIFIC COMPONENT'S DESCRIPTION\n",
    "# 13      CITY              CHAR(30)      CONSUMER'S CITY\n",
    "# 14      STATE             CHAR(2)       CONSUMER'S STATE CODE\n",
    "# 15      VIN               CHAR(11)      VEHICLE'S VIN#\n",
    "# 16      DATEA             CHAR(8)       DATE ADDED TO FILE (YYYYMMDD)\n",
    "# 17      LDATE             CHAR(8)       DATE COMPLAINT RECEIVED BY NHTSA (YYYYMMDD)\n",
    "# 18      MILES             NUMBER(7)     VEHICLE MILEAGE AT FAILURE\n",
    "# 19      OCCURENCES        NUMBER(4)     NUMBER OF OCCURRENCES\n",
    "# 20      CDESCR            CHAR(2048)    DESCRIPTION OF THE COMPLAINT\n",
    "# 21      CMPL_TYPE         CHAR(4)       SOURCE OF COMPLAINT CODE:\n",
    "#                                           CAG  =CONSUMER ACTION GROUP\n",
    "#                                           CON  =FORWARDED FROM A CONGRESSIONAL OFFICE\n",
    "#                                           DP   =DEFECT PETITION,RESULT OF A DEFECT PETITION\n",
    "#                                           EVOQ =HOTLINE VOQ\n",
    "#                                           EWR  =EARLY WARNING REPORTING\n",
    "#                                           INS  =INSURANCE COMPANY\n",
    "#                                           IVOQ =NHTSA WEB SITE\n",
    "#                                           LETR =CONSUMER LETTER\n",
    "#                                           MAVQ =NHTSA MOBILE APP\n",
    "#                                           MIVQ =NHTSA MOBILE APP\n",
    "#                                           MVOQ =OPTICAL MARKED VOQ\n",
    "#                                           RC   =RECALL COMPLAINT,RESULT OF A RECALL INVESTIGATION\n",
    "#                                           RP   =RECALL PETITION,RESULT OF A RECALL PETITION\n",
    "#                                           SVOQ =PORTABLE SAFETY COMPLAINT FORM (PDF)\n",
    "#                                           VOQ  =NHTSA VEHICLE OWNERS QUESTIONNAIRE\n",
    "# 22      POLICE_RPT_YN     CHAR(1)       WAS INCIDENT REPORTED TO POLICE 'Y' OR 'N'\n",
    "# 23      PURCH_DT          CHAR(8)       DATE PURCHASED (YYYYMMDD)\n",
    "# 24      ORIG_OWNER_YN     CHAR(1)       WAS ORIGINAL OWNER 'Y' OR 'N'\n",
    "# 25      ANTI_BRAKES_YN    CHAR(1)       ANTI-LOCK BRAKES 'Y' OR 'N'\n",
    "# 26      CRUISE_CONT_YN    CHAR(1)       CRUISE CONTROL 'Y' OR 'N'\n",
    "# 27      NUM_CYLS          NUMBER(2)     NUMBER OF CYLINDERS\n",
    "# 28      DRIVE_TRAIN       CHAR(4)       DRIVE TRAIN TYPE [AWD,4WD,FWD,RWD]\n",
    "# 29      FUEL_SYS          CHAR(4)       FUEL SYSTEM CODE:\n",
    "#                                            FI =FUEL INJECTION\n",
    "#                                            TB =TURBO\n",
    "# 30      FUEL_TYPE         CHAR(4)       FUEL TYPE CODE:\n",
    "#                                            BF =BIFUEL\n",
    "#                                            CN =CNG/LPG\n",
    "#                                            DS =DIESEL\n",
    "#                                            GS =GAS\n",
    "#                                            HE =HYBRID ELECTRIC\n",
    "# 31      TRANS_TYPE        CHAR(4)       VEHICLE TRANSMISSION TYPE [AUTO, MAN]\n",
    "# 32      VEH_SPEED         NUMBER(3)     VEHICLE SPEED\n",
    "# 33      DOT               CHAR(20)      DEPARTMENT OF TRANSPORTATION TIRE IDENTIFIER\n",
    "# 34      TIRE_SIZE         CHAR(30)      TIRE SIZE\n",
    "# 35      LOC_OF_TIRE       CHAR(4)       LOCATION OF TIRE CODE:\n",
    "#                                            FSW =DRIVER SIDE FRONT\n",
    "#                                            DSR =DRIVER SIDE REAR\n",
    "#                                            FTR =PASSENGER SIDE FRONT\n",
    "#                                            PSR =PASSENGER SIDE REAR\n",
    "#                                            SPR =SPARE\n",
    "# 36      TIRE_FAIL_TYPE    CHAR(4)       TYPE OF TIRE FAILURE CODE:\n",
    "#                                            BST =BLISTER\n",
    "#                                            BLW =BLOWOUT\n",
    "#                                            TTL =CRACK\n",
    "#                                            OFR =OUT OF ROUND\n",
    "#                                            TSW =PUNCTURE\n",
    "#                                            TTR =ROAD HAZARD\n",
    "#                                            TSP =TREAD SEPARATION\n",
    "# 37      ORIG_EQUIP_YN     CHAR(1)       WAS PART ORIGINAL EQUIPMENT 'Y' OR 'N'\n",
    "# 38      MANUF_DT          CHAR(8)       DATE OF MANUFACTURE (YYYYMMDD)\n",
    "# 39      SEAT_TYPE         CHAR(4)       TYPE OF CHILD SEAT CODE:\n",
    "#                                            B  =BOOSTER\n",
    "#                                            C  =CONVERTIBLE\n",
    "#                                            I  =INFANT\n",
    "#                                            IN =INTEGRATED\n",
    "#                                            TD =TODDLER\n",
    "# 40     RESTRAINT_TYPE     CHAR(4)       INSTALLATION SYSTEM CODE;\n",
    "#                                            A =VEHICLE SAFETY BELT\n",
    "#                                            B =LATCH SYSTEM\n",
    "# 41     DEALER_NAME        CHAR(40)      DEALER'S NAME\n",
    "# 42     DEALER_TEL         CHAR(20)      DEALER'S TELEPHONE NUMBER\n",
    "# 43     DEALER_CITY        CHAR(30)      DEALER'S CITY\n",
    "# 44     DEALER_STATE       CHAR(2)       DEALER'S STATE CODE\n",
    "# 45     DEALER_ZIP         CHAR(10)      DEALER'S ZIPCODE\n",
    "# 46     PROD_TYPE          CHAR(4)       PRODUCT TYPE CODE:\n",
    "#                                            V =VEHICLE\n",
    "#                                            T =TIRES\n",
    "#                                            E =EQUIPMENT\n",
    "#                                            C =CHILD RESTRAINT\n",
    "# 47     REPAIRED_YN        CHAR(1)       WAS DEFECTIVE TIRE REPAIRED 'Y' OR 'N'\n",
    "# 48     MEDICAL_ATTN       CHAR(1)       WAS MEDICAL ATTENTION REQUIRED 'Y' OR 'N'\n",
    "# 49     VEHICLES_TOWED_YN  CHAR(1)       WAS VEHICLE TOWED 'Y' OR 'N'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# https://www.nhtsa.gov/nhtsa-datasets-and-apis#recalls\n",
    "filename = \"C:\\\\Repo\\\\SIADs_Audio_Text_SRS\\\\Datasets\\\\COMPLAINTS_RECEIVED_2025-2025.txt\"\n",
    "# read in C:\\Repo\\SIADs_Audio_Text_SRS\\Example\\COMPLAINTS_RECEIVED_2025-2025.txt into a pandas dataframe, where the columns are RCL\n",
    "df_complaints = pd.read_csv(filename, sep='\\t', header=None, index_col=0)\n",
    "df_complaints.columns = ['ODINO', 'MFR_NAME', 'MAKETXT', 'MODELTXT', 'YEARTXT', 'CRASH', 'FAILDATE', 'FIRE', 'INJURED', 'DEATHS', 'COMPDESC', 'CITY', 'STATE', 'VIN', 'DATEA', 'LDATE', 'MILES', 'OCCURENCES', 'CDESCR', 'CMPL_TYPE', 'POLICE_RPT_YN', 'PURCH_DT', 'ORIG_OWNER_YN', 'ANTI_BRAKES_YN', 'CRUISE_CONT_YN', 'NUM_CYLS', 'DRIVE_TRAIN', 'FUEL_SYS', 'FUEL_TYPE',\n",
    "              'TRANS_TYPE', 'VEH_SPEED', 'DOT', 'TIRE_SIZE', 'LOC_OF_TIRE', 'TIRE_FAIL_TYPE', 'ORIG_EQUIP_YN', 'MANUF_DT', 'SEAT_TYPE', 'RESTRAINT_TYPE', 'DEALER_NAME', 'DEALER_TEL', 'DEALER_CITY', 'DEALER_STATE', 'DEALER_ZIP', 'PROD_TYPE', 'REPAIRED_YN', 'MEDICAL_ATTN', 'VEHICLES_TOWED_YN']\n",
    "\n",
    "summary = df_complaints.describe()\n",
    "display(summary)\n",
    "\n",
    "deadly_complaints = df_complaints[df_complaints[\"DEATHS\"] > 0]\n",
    "#print(deadly_complaints)\n",
    "\n",
    "# print any of the complaints that lead to a death\n",
    "display(deadly_complaints[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"DEATHS\", \"CDESCR\"]])\n",
    "# print the tesla complaints that lead to a death\n",
    "print(deadly_complaints[deadly_complaints[\"MFR_NAME\"] == \"Tesla, Inc.\"][\"CDESCR\"].values)\n",
    "\n",
    "# display columns 11 through 25\n",
    "#display(df_complaints)\n",
    "\n",
    "sub_df = df_complaints[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]]\n",
    "display(sub_df)\n",
    "#display(read_complaints_received())\n",
    "# print all the unique COMPDESC values\n",
    "COMPDESC_list = df_complaints[\"COMPDESC\"].unique()\n",
    "print(COMPDESC_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECALLS\n",
    "\n",
    "# The RECALL file contains all NHTSA safety-related defect and compliance \n",
    "# campaigns since 1967.\n",
    "\n",
    "# File characteristics:\n",
    "\n",
    "# -  All the records are TAB delimited\n",
    "# -  All dates are in YYYYMMDD format\n",
    "\n",
    "# -  Maximum Record length: 9109\n",
    "\n",
    "# Change log:\n",
    "# 1.Field# 23 added as of Sept. 14, 2007\n",
    "# 2.Changed flat file extension from .lst to .txt as of Sept. 14, 2007\n",
    "# 3.Field# 24 added as of March 14, 2008\n",
    "# 4.Field#s 25,26,27 added as of March 23, 2020\n",
    "\n",
    "# Last Updated March 23, 2020\n",
    "\n",
    "\n",
    "# FIELDS:\n",
    "# =======\n",
    "\n",
    "# Field#   Name                Type/Size   Description                      \n",
    "# ------   ---------           ---------   --------------------------------------\n",
    "# 1        RECORD_ID           NUMBER(9)   RUNNING SEQUENCE NUMBER, \n",
    "#                                           WHICH UNIQUELY IDENTIFIES THE RECORD.\n",
    "# 2        CAMPNO              CHAR(12)    NHTSA CAMPAIGN NUMBER\n",
    "# 3        MAKETXT             CHAR(25)    VEHICLE/EQUIPMENT MAKE\n",
    "# 4        MODELTXT            CHAR(256)   VEHICLE/EQUIPMENT MODEL\n",
    "# 5        YEARTXT             CHAR(4)     MODEL YEAR, 9999 IF UNKNOWN or N/A\n",
    "# 6        MFGCAMPNO           CHAR(20)    MFR CAMPAIGN NUMBER\n",
    "# 7        COMPNAME            CHAR(256)   COMPONENT DESCRIPTION\n",
    "# 8        MFGNAME             CHAR(40)    MANUFACTURER THAT FILED DEFECT/NONCOMPLIANCE REPORT\n",
    "# 9        BGMAN               CHAR(8)     BEGIN DATE OF MANUFACTURING\n",
    "# 10       ENDMAN              CHAR(8)     END DATE OF MANUFACTURING\n",
    "# 11       RCLTYPECD           CHAR(4)     VEHICLE, EQUIPMENT OR TIRE REPORT\n",
    "# 12       POTAFF              NUMBER(9)   POTENTIAL NUMBER OF UNITS AFFECTED               \n",
    "# 13       ODATE               CHAR(8)     DATE OWNER NOTIFIED BY MFR\n",
    "# 14       INFLUENCED_BY       CHAR(4)     RECALL INITIATOR (MFR/OVSC/ODI)\n",
    "# 15       MFGTXT              CHAR(40)    MANUFACTURERS OF RECALLED VEHICLES/PRODUCTS\n",
    "# 16       RCDATE              CHAR(8)     REPORT RECEIVED DATE\n",
    "# 17       DATEA               CHAR(8)     RECORD CREATION DATE\n",
    "# 18       RPNO                CHAR(3)     REGULATION PART NUMBER\n",
    "# 19       FMVSS               CHAR(10)    FEDERAL MOTOR VEHICLE SAFETY STANDARD NUMBER\n",
    "# 20       DESC_DEFECT         CHAR(2000)  DEFECT SUMMARY\n",
    "# 21       CONEQUENCE_DEFECT   CHAR(2000)  CONSEQUENCE SUMMARY\t\n",
    "# 22       CORRECTIVE_ACTION   CHAR(2000)  CORRECTIVE SUMMARY\n",
    "# 23       NOTES               CHAR(2000)  RECALL NOTES\n",
    "# 24       RCL_CMPT_ID         CHAR(27)    NUMBER THAT UNIQUELY IDENTIFIES A RECALLED COMPONENT.\n",
    "# 25       MFR_COMP_NAME       CHAR(50)    MANUFACTURER-SUPPLIED COMPONENT NAME\n",
    "# 26       MFR_COMP_DESC       CHAR(200)   MANUFACTURER-SUPPLIED COMPONENT DESCRIPTION\n",
    "# 27       MFR_COMP_PTNO       CHAR(100)   MANUFACTURER-SUPPLIED COMPONENT PART NUMBER\n",
    "\n",
    "# read in C:\\Repo\\SIADs_Audio_Text_SRS\\Datasets\\FLAT_RCL.txt\n",
    "# there are only 24 columns in the file, so we need to specify the column names\n",
    "# https://www.nhtsa.gov/nhtsa-datasets-and-apis#recalls\n",
    "df_recall = pd.read_csv(\"C:\\\\Repo\\\\SIADs_Audio_Text_SRS\\\\Datasets\\\\FLAT_RCL.txt\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "# use the column names listed above\n",
    "df_recall.columns = ['RECORD_ID', 'CAMPNO', 'MAKETXT', 'MODELTXT', 'YEARTXT', 'MFGCAMPNO', 'COMPNAME', 'MFGNAME', 'BGMAN', 'ENDMAN', 'RCLTYPECD', 'POTAFF', 'ODATE', 'INFLUENCED_BY', 'MFGTXT', 'RCDATE', 'DATEA', 'RPNO', 'FMVSS', 'DESC_DEFECT', 'CONEQUENCE_DEFECT', 'CORRECTIVE_ACTION', 'NOTES', 'RCL_CMPT_ID', 'MFR_COMP_NAME', 'MFR_COMP_DESC', 'MFR_COMP_PTNO']\n",
    "display(df_recall)\n",
    "\n",
    "# print all the unique COMPDESC values\n",
    "COMPNAME_list = df_recall[\"COMPNAME\"].unique()\n",
    "print(COMPNAME_list)\n",
    "print(len(COMPNAME_list))\n",
    "# state encode the COMPDESC values and create a new column in the dataframe called COMPDESC_StateEncoded\n",
    "df_recall[\"COMPNAME_StateEncoded\"] = df_recall[\"COMPNAME\"].apply(lambda x: hash(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# create a function that will be called from a apply lambda function to process the text from a dataframe with the \"CDESCR\" column\n",
    "def process_text(text):\n",
    "    \"\"\"\n",
    "    Process text by tokenizing, removing stop words, and stemming\n",
    "    \"\"\"\n",
    "    #print(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # make sure the text is not empty and or nan\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "\n",
    "    # tokenize the text and remove stop words and stem the words\n",
    "    content_cleaned = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word not in stop_words]\n",
    "\n",
    "    # remove the punctuation from the content_cleaned list\n",
    "    content_cleaned = [word for word in content_cleaned if word.isalnum()]\n",
    "    \n",
    "    return content_cleaned\n",
    "\n",
    "# process the text in the \"CDESCR\" column and create a new column \"CDESCR_CLEANED\" with the processed text\n",
    "df_complaints[\"CDESCR_CLEANED\"] = df_complaints[\"CDESCR\"].apply(lambda x: process_text(x))\n",
    "display(df_complaints[[\"CDESCR\", \"CDESCR_CLEANED\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most common words in the \"CDESCR_CLEANED\" column\n",
    "from collections import Counter\n",
    "\n",
    "# create a Counter object from the \"CDESCR_CLEANED\" column\n",
    "word_counter = Counter([word for words in df_complaints[\"CDESCR_CLEANED\"] for word in words])\n",
    "\n",
    "# get the 10 most common words\n",
    "most_common_words = word_counter.most_common(10)\n",
    "print(most_common_words)\n",
    "\n",
    "# get the 10 least common words\n",
    "least_common_words = word_counter.most_common()[:-10:-1]\n",
    "print(least_common_words)\n",
    "\n",
    "# get the 10 most common words in the \"CDESCR_CLEANED\" column for complaints that lead to a death\n",
    "deadly_complaints = df_complaints[df_complaints[\"DEATHS\"] > 0]\n",
    "word_counter_deadly = Counter([word for words in deadly_complaints[\"CDESCR_CLEANED\"] for word in words])\n",
    "most_common_words_deadly = word_counter_deadly.most_common(10)\n",
    "print(most_common_words_deadly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nan values from the \"CDESCR\" column and put it into a new dataframe named \"df_complaints_no_nan\"\n",
    "df_complaints_no_nan = df_complaints.dropna(subset=[\"CDESCR\"])\n",
    "# find any instances of the word \"diagnostic\" or \"DTC\" in the \"CDESCR\" column, DTC stands for Diagnostic Trouble Code and is used in the automotive industry for identifying issues with a vehicle\n",
    "diagnostic_complaints = df_complaints_no_nan[df_complaints_no_nan[\"CDESCR\"].str.contains(\"diagnostic|DTC\", case=False)]\n",
    "display(diagnostic_complaints[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\", \"CDESCR_CLEANED\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the df_complaints dataframe into a test, train, and validation set with a 70/20/10 split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = 42\n",
    "train_size = 0.7\n",
    "test_size = 0.2\n",
    "validation_size = 0.1\n",
    "\n",
    "\n",
    "train, test = train_test_split(df_complaints, test_size=(1-train_size), random_state=random_state)\n",
    "test, validation = train_test_split(test, test_size=(validation_size/(1-train_size)), random_state=random_state)\n",
    "print(df_complaints.shape)\n",
    "print(train.shape, test.shape, validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize the \"CDESCR_CLEANED\" column using the TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create a TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "# set the number of dimensions to reduce the vectorized data to\n",
    "num_dimensions = 100\n",
    "\n",
    "\n",
    "# fit the vectorizer on the \"CDESCR_CLEANED\" column and transform the \"CDESCR_CLEANED\" column into a vectorized format\n",
    "# apply a lambda function to join the list of words in the \"CDESCR_CLEANED\" column into a string\n",
    "X_train = vectorizer.fit_transform(train[\"CDESCR_CLEANED\"].apply(lambda x: \" \".join(x)))\n",
    "X_test = vectorizer.transform(test[\"CDESCR_CLEANED\"].apply(lambda x: \" \".join(x)))\n",
    "X_validation = vectorizer.transform(validation[\"CDESCR_CLEANED\"].apply(lambda x: \" \".join(x)))\n",
    "print(X_train.shape, X_test.shape, X_validation.shape)\n",
    "\n",
    "# perform LSA on the vectorized data to reduce the dimensionality\n",
    "lsa = TruncatedSVD(n_components=num_dimensions, random_state=random_state)\n",
    "complaints_vectorized_train = lsa.fit_transform(X_train)\n",
    "vectorized_test = lsa.transform(X_test)\n",
    "complaints_vectorized_validation = lsa.transform(X_validation)\n",
    "print(complaints_vectorized_train.shape, vectorized_test.shape, complaints_vectorized_validation.shape)\n",
    "# print out the words that correspond to the first 10 dimensions of the LSA\n",
    "\n",
    "# create a list of unique manufacturers in the \"MFR_NAME\" column\n",
    "list_of_manufacturers = df_complaints[\"MFR_NAME\"].unique()\n",
    "#print(list_of_manufacturers)\n",
    "\n",
    "# Find the cosine similarity between the first complaint in the test set and all the complaints in the training set\n",
    "# get the first complaint in the test set\n",
    "complaint_test = vectorized_test[5].reshape(1, -1)\n",
    "# get the cosine similarity between the first complaint in the test set and all the complaints in the training set\n",
    "cosine_similarities = cosine_similarity(complaint_test, complaints_vectorized_train)\n",
    "# get the index of the most similar complaint in the training set\n",
    "most_similar_index = cosine_similarities.argmax()\n",
    "print(most_similar_index)\n",
    "# get the most similar complaint in the training set\n",
    "most_similar_complaint_train = train.iloc[most_similar_index]\n",
    "print(most_similar_complaint_train[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint_train[\"CDESCR\"])\n",
    "# get the most similar complaint in the test set\n",
    "most_similar_complaint_test = test.iloc[5]\n",
    "print(most_similar_complaint_test[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint_test[\"CDESCR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query text to vectorize and find similar complaints to\n",
    "query_text = \"EV not charging\"\n",
    "\n",
    "# process the query text\n",
    "query_text_cleaned = process_text(query_text)\n",
    "# vectorize the query text\n",
    "query_vectorized = vectorizer.transform([\" \".join(query_text_cleaned)])\n",
    "# reduce the dimensionality of the query vector\n",
    "query_vectorized_lsa = lsa.transform(query_vectorized)\n",
    "# find the cosine similarity between the query vector and all the complaints in the training set\n",
    "cosine_similarities_query = cosine_similarity(query_vectorized_lsa, complaints_vectorized_train)\n",
    "# get the index of the most similar complaint in the training set\n",
    "most_similar_index_query = cosine_similarities_query.argmax()\n",
    "# get the most similar complaint in the training set\n",
    "most_similar_complaint_train_query = train.iloc[most_similar_index_query]\n",
    "print(most_similar_complaint_train_query[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint_train_query[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionalize the process of finding similar complaints to a query text\n",
    "def find_similar_complaint(query_text:str, vectorizer, lsa, train):\n",
    "    \"\"\"\n",
    "    Find the most similar complaint to a query text in the training set\n",
    "    \"\"\"\n",
    "    # process the query text\n",
    "    query_text_cleaned = process_text(query_text)\n",
    "    # vectorize the query text\n",
    "    query_vectorized = vectorizer.transform([\" \".join(query_text_cleaned)])\n",
    "    # reduce the dimensionality of the query vector\n",
    "    query_vectorized_lsa = lsa.transform(query_vectorized)\n",
    "    # find the cosine similarity between the query vector and all the complaints in the training set\n",
    "    cosine_similarities_query = cosine_similarity(query_vectorized_lsa, complaints_vectorized_train)\n",
    "    # get the index of the most similar complaint in the training set\n",
    "    most_similar_index_query = cosine_similarities_query.argmax()\n",
    "    # get the most similar complaint in the training set\n",
    "    most_similar_complaint_train_query = train.iloc[most_similar_index_query]\n",
    "    return most_similar_complaint_train_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar_stuff = \"rattling noise when driving slowly\"\n",
    "most_similar_complaint = find_similar_complaint(find_similar_stuff, vectorizer, lsa, train)\n",
    "print(most_similar_complaint[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class TextClassifier:\n",
    "    def __init__(self, df, column_name:str):\n",
    "        # set the random state for reproducibility\n",
    "        self.random_state = 42\n",
    "        # create a TfidfVectorizer object\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        # set the number of dimensions to reduce the vectorized data to\n",
    "        self.num_dimensions = 100\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.column_name_cleaned = column_name + \"_CLEANED\"\n",
    "        # create variables to store the training, test, and validation sets for class functions\n",
    "        self.column_name_cleaned = None\n",
    "        self.df_train = None\n",
    "        self.df_test = None\n",
    "        self.df_validation = None\n",
    "        self.x_train_vect = None\n",
    "        self.x_test_vect = None\n",
    "        self.x_validation_vect = None\n",
    "        self.lsa = None\n",
    "        self.vectorized_train = None\n",
    "        self.vectorized_test = None\n",
    "        self.vectorized_validation = None\n",
    "\n",
    "    # create a function that will be called from a apply lambda function to process the text from a dataframe with the \"CDESCR\" column\n",
    "    def process_text(text):\n",
    "        \"\"\"\n",
    "        Process text by tokenizing, removing stop words, and stemming\n",
    "        \"\"\"\n",
    "        #print(text)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "        # make sure the text is not empty and or nan\n",
    "        if not text or pd.isna(text):\n",
    "            return []\n",
    "\n",
    "        # tokenize the text and remove stop words and stem the words\n",
    "        content_cleaned = [stemmer.stem(word) for word in word_tokenize(text.lower()) if word not in stop_words]\n",
    "\n",
    "        # remove the punctuation from the content_cleaned list\n",
    "        content_cleaned = [word for word in content_cleaned if word.isalnum()]\n",
    "        \n",
    "        return content_cleaned\n",
    "    \n",
    "    def process_dataframe(self, train_size=0.7, test_size=0.2, validation_size=0.1):\n",
    "        \"\"\"\n",
    "        Process the text in the \"CDESCR\" column and create a new column \"CDESCR_CLEANED\" with the processed text\n",
    "        \"\"\"\n",
    "        # get current working directory\n",
    "        cwd = os.getcwd()\n",
    "        # change the Example folder to Datasets folder in the cwd path\n",
    "        desired_save_path = cwd.replace(\"Example\", \"Datasets\")\n",
    "        # create a folder path to save the pickle files\n",
    "        if not os.path.exists(desired_save_path):\n",
    "            os.makedirs(desired_save_path)\n",
    "\n",
    "        # check to see if there is a pickle file for the dataframe with the processed text\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_df.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_df.pkl\", \"rb\") as f:\n",
    "                self.df = pickle.load(f)\n",
    "        else:\n",
    "            # process the text in the \"CDESCR\" column and create a new column \"CDESCR_CLEANED\" with the processed text\n",
    "            self.df[self.column_name_cleaned] = self.df[self.column_name].apply(lambda x: TextClassifier.process_text(x))\n",
    "            # create a pickle file for the dataframe with the processed text\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_df.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.df, f)\n",
    "\n",
    "        # split the df_complaints dataframe into a test, train, and validation set with a 70/20/10 split\n",
    "        self.df_train, self.df_test = train_test_split(self.df, test_size=(1-train_size), random_state=self.random_state)\n",
    "        self.df_test, self.df_validation = train_test_split(self.df_test, test_size=(validation_size/(1-train_size)), random_state=self.random_state)\n",
    "\n",
    "        # check to see if there is a pickle file for the vectorizer\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorizer.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorizer.pkl\", \"rb\") as f:\n",
    "                self.vectorizer = pickle.load(f)\n",
    "        else:\n",
    "            # fit the vectorizer on the \"CDESCR_CLEANED\" column and transform the \"CDESCR_CLEANED\" column into a vectorized format\n",
    "            # apply a lambda function to join the list of words in the \"CDESCR_CLEANED\" column into a string\n",
    "            self.x_train_vect = self.vectorizer.fit_transform(self.df_train[self.column_name_cleaned].apply(lambda x: \" \".join(x)))\n",
    "            self.x_test_vect = self.vectorizer.transform(self.df_test[self.column_name_cleaned].apply(lambda x: \" \".join(x)))\n",
    "            self.x_validation_vect = self.vectorizer.transform(self.df_validation[self.column_name_cleaned].apply(lambda x: \" \".join(x)))        \n",
    "            #print(self.x_train_vect.shape, self.x_test_vect.shape, self.x_validation_vect.shape)\n",
    "            # create a pickle file for the vectorizer\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorizer.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.vectorizer, f)\n",
    "\n",
    "        # check to see if there is a pickle file for the lsa\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_lsa.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_lsa.pkl\", \"rb\") as f:\n",
    "                self.lsa = pickle.load(f)\n",
    "        else:\n",
    "            # perform LSA on the vectorized data to reduce the dimensionality\n",
    "            self.lsa = TruncatedSVD(n_components=self.num_dimensions, random_state=self.random_state)\n",
    "\n",
    "        # check to see if there is a pickle file for the vectorized training data\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorized_train.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_train.pkl\", \"rb\") as f:\n",
    "                self.complaints_vectorized_train = pickle.load(f)\n",
    "        else:\n",
    "            self.complaints_vectorized_train = self.lsa.fit_transform(self.x_train_vect)\n",
    "            # create a pickle file for the vectorized training data\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_train.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.complaints_vectorized_train, f)\n",
    "        \n",
    "        # check to see if there is a pickle file for the vectorized test data\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorized_test.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_test.pkl\", \"rb\") as f:\n",
    "                self.vectorized_test = pickle.load(f)\n",
    "        else:\n",
    "            self.vectorized_test = self.lsa.transform(self.x_test_vect)\n",
    "            # create a pickle file for the vectorized test data\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_test.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.vectorized_test, f)\n",
    "\n",
    "        # check to see if there is a pickle file for the vectorized validation data\n",
    "        if os.path.exists(desired_save_path + '//' + self.column_name + \"_vectorized_validation.pkl\"):\n",
    "            # load the pickle file\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_validation.pkl\", \"rb\") as f:\n",
    "                self.complaints_vectorized_validation = pickle.load(f)\n",
    "        else:\n",
    "            self.complaints_vectorized_validation = self.lsa.transform(self.x_validation_vect)\n",
    "            # create a pickle file for the vectorized validation data\n",
    "            with open(desired_save_path + '//' + self.column_name + \"_vectorized_validation.pkl\", \"wb\") as f:\n",
    "                pickle.dump(self.complaints_vectorized_validation, f)\n",
    "\n",
    "        return self.df, self.complaints_vectorized_train, self.vectorized_test, self.complaints_vectorized_validation, self.df_train, self.df_test, self.df_validation\n",
    "    \n",
    "    # A functionalize the process of finding similar complaints to a query text\n",
    "    def find_similar_complaint(self, query_text:str):\n",
    "        \"\"\"\n",
    "        Find the most similar complaint to a query text in the training set\n",
    "        \"\"\"\n",
    "        # process the query text\n",
    "        query_text_cleaned = TextClassifier.process_text(query_text)\n",
    "        # vectorize the query text\n",
    "        query_vectorized = self.vectorizer.transform([\" \".join(query_text_cleaned)])\n",
    "        # reduce the dimensionality of the query vector\n",
    "        query_vectorized_lsa = self.lsa.transform(query_vectorized)\n",
    "        # find the cosine similarity between the query vector and all the complaints in the training set\n",
    "        cosine_similarities_query = cosine_similarity(query_vectorized_lsa, self.complaints_vectorized_train)\n",
    "        # get the index of the most similar complaint in the training set\n",
    "        most_similar_index_query = cosine_similarities_query.argmax()\n",
    "        print(most_similar_index_query)\n",
    "        # get the most similar complaint in the training set\n",
    "        most_similar_complaint_train_query = self.df_train.iloc[most_similar_index_query]\n",
    "        return most_similar_complaint_train_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nhtsa.gov/nhtsa-datasets-and-apis#recalls\n",
    "# read in C:\\Repo\\SIADs_Audio_Text_SRS\\Example\\COMPLAINTS_RECEIVED_2025-2025.txt into a pandas dataframe, where the columns are RCL\n",
    "df_complaints = pd.read_csv(\"C:\\\\Repo\\\\SIADs_Audio_Text_SRS\\\\Datasets\\\\COMPLAINTS_RECEIVED_2025-2025.txt\", sep='\\t', header=None, index_col=0)\n",
    "df_complaints.columns = ['ODINO', 'MFR_NAME', 'MAKETXT', 'MODELTXT', 'YEARTXT', 'CRASH', 'FAILDATE', 'FIRE', 'INJURED', 'DEATHS', 'COMPDESC', 'CITY', 'STATE', 'VIN', 'DATEA', 'LDATE', 'MILES', 'OCCURENCES', 'CDESCR', 'CMPL_TYPE', 'POLICE_RPT_YN', 'PURCH_DT', 'ORIG_OWNER_YN', 'ANTI_BRAKES_YN', 'CRUISE_CONT_YN', 'NUM_CYLS', 'DRIVE_TRAIN', 'FUEL_SYS', 'FUEL_TYPE',\n",
    "              'TRANS_TYPE', 'VEH_SPEED', 'DOT', 'TIRE_SIZE', 'LOC_OF_TIRE', 'TIRE_FAIL_TYPE', 'ORIG_EQUIP_YN', 'MANUF_DT', 'SEAT_TYPE', 'RESTRAINT_TYPE', 'DEALER_NAME', 'DEALER_TEL', 'DEALER_CITY', 'DEALER_STATE', 'DEALER_ZIP', 'PROD_TYPE', 'REPAIRED_YN', 'MEDICAL_ATTN', 'VEHICLES_TOWED_YN']\n",
    "\n",
    "# create a list of unique manufacturers in the \"MFR_NAME\" column\n",
    "list_of_manufacturers = df_complaints[\"MFR_NAME\"].unique()\n",
    "\n",
    "# testing out the functionilzed code\n",
    "\n",
    "# call the TextClassifier class and create an instance of it as text_classifier\n",
    "# pass in the df_complaints dataframe and the \"CDESCR\" column\n",
    "text_classifier = TextClassifier(df_complaints, \"CDESCR\")\n",
    "# process the text in the \"CDESCR\" column\n",
    "text_classifier.process_dataframe()\n",
    "\n",
    "# use one of the complaints in the test set as a query to find the most similar complaint in the training set\n",
    "complaint_test_query = text_classifier.df_test[\"CDESCR\"].iloc[5]\n",
    "print(complaint_test_query)\n",
    "# find the most similar complaint to the complaint test\n",
    "most_similar_complaint = text_classifier.find_similar_complaint(complaint_test_query)\n",
    "# print the most similar complaint with the below columns\n",
    "print(most_similar_complaint[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most similar complaint to the complaint test\n",
    "most_similar_complaint = text_classifier.find_similar_complaint(\"Car won't start\")# and makes a clicking noise\")\n",
    "# print the most similar complaint with the below columns\n",
    "print(most_similar_complaint[[\"ODINO\", \"MFR_NAME\", \"MAKETXT\", \"MODELTXT\", \"YEARTXT\", \"CDESCR\"]])\n",
    "print(most_similar_complaint[\"CDESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording audio...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Output/complaint.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# call the record_audio function to record audio from the microphone and save it to a file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording audio...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mrecord_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# call the extract_text_from_audio function to extract text from the audio file and save it to a file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Repo\\SIADs_Audio_Text_SRS\\Example\\Extracted_Text.py:147\u001b[0m, in \u001b[0;36mrecord_audio\u001b[1;34m(output_file, duration, freq)\u001b[0m\n\u001b[0;32m    144\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput/complaint.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# You can change the file name\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Convert the NumPy array to audio file\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecording\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Repo\\SIADs_Audio_Text_SRS\\env\\lib\\site-packages\\wavio.py:472\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(file, data, rate, scale, sampwidth, clip)\u001b[0m\n\u001b[0;32m    468\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    470\u001b[0m wavdata \u001b[38;5;241m=\u001b[39m _array2wav(data, sampwidth)\n\u001b[1;32m--> 472\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[43m_wave\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m w\u001b[38;5;241m.\u001b[39msetnchannels(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    474\u001b[0m w\u001b[38;5;241m.\u001b[39msetsampwidth(sampwidth)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\wave.py:511\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Wave_read(f)\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWave_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\wave.py:304\u001b[0m, in \u001b[0;36mWave_write.__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_opened_the_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 304\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_i_opened_the_file \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Output/complaint.wav'"
     ]
    }
   ],
   "source": [
    "from Extracted_Text import record_audio\n",
    "from Extracted_Text import wav_to_text_and_tokenize\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# file name \n",
    "filename = \"C:\\\\Repo\\\\SIADs_Audio_Text_SRS\\\\Datasets\\\\audio.wav\"\n",
    "\n",
    "# call the record_audio function to record audio from the microphone and save it to a file\n",
    "print(\"Recording audio...\")\n",
    "record_audio(filename)\n",
    "print(\"Recording finished.\")\n",
    "# call the extract_text_from_audio function to extract text from the audio file and save it to a file\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    tokens = wav_to_text_and_tokenize(filename)\n",
    "\n",
    "    if tokens is not None:\n",
    "        if tokens:\n",
    "            print(\"Tokens:\", tokens)\n",
    "\n",
    "            # Stop word removal (example)\n",
    "            from nltk.corpus import stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            filtered_tokens = [w for w in tokens if not w.lower() in stop_words]\n",
    "            print(\"Filtered Tokens (Stop words removed):\", filtered_tokens)\n",
    "        else:\n",
    "            print(\"No text was recognized.\")\n",
    "    else:\n",
    "        print(\"An error occurred during processing.\")\n",
    "else:\n",
    "    print(f\"File not found: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
